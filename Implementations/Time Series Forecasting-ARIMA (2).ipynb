{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    " \n",
    "    \n",
    "sys.path.insert(1, '../../tsad/')\n",
    "from tsad.useful.ts import ts_train_test_split     \n",
    "from tsad.models.fit import fit, set_determenistic\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from tsad.models.lstm import SimpleLSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tsad.useful.iterators import Loader\n",
    "    \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63c2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3ead9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'skab.zip'\n",
    "if not os.path.exists('skab.zip'):\n",
    "    import requests\n",
    "    url = 'https://github.com/waico/SKAB/archive/refs/heads/master.zip'\n",
    "    r = requests.get(url)\n",
    "    open(fname , 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d547419",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(fname)\n",
    "train_datasets = []\n",
    "test_datasets  = []\n",
    "for file  in z.namelist():\n",
    "    if '.csv' in file:\n",
    "        if 'anomaly-free' in file:\n",
    "            test_datasets.append(pd.read_csv(z.open(file),index_col='datetime',sep=';',parse_dates=True))\n",
    "        else:\n",
    "            train_datasets.append(pd.read_csv(z.open(file),index_col='datetime',sep=';',parse_dates=True).drop(columns= ['anomaly','changepoint']))\n",
    "\n",
    "scaler = StandardScaler().fit(pd.concat(train_datasets+test_datasets))\n",
    "train_datasets = [pd.DataFrame(scaler.transform(df),index=df.index,columns=df.columns) for df in train_datasets]\n",
    "test_datasets = [pd.DataFrame(scaler.transform(df),index=df.index,columns=df.columns) for df in test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51cb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7fd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb823e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fc5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = []\n",
    "y_train_ts = []\n",
    "for df in train_datasets:\n",
    "    sets = ts_train_test_split(df,window,test_size=0)\n",
    "    X_train_ts+=sets[0]\n",
    "    y_train_ts+=sets[2]\n",
    "\n",
    "sets = ts_train_test_split(test_datasets[0],window,test_size=0)\n",
    "X_test_ts=sets[0]\n",
    "y_test_ts=sets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533ff44",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f8e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d81de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9375/9375 [26:11:26<00:00, 10.06s/it]\n",
      " 84%|█████████████████████████████████████████████████████████████▌           | 7912/9375 [19:20:49<5:15:51, 12.95s/it]"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for col in df:\n",
    "    for i in tqdm(range(window,len(df))):\n",
    "        try:\n",
    "            ts = df[col][:i]\n",
    "            model = ARIMA(ts[:i], order=(30,1,0))\n",
    "            res = model.fit()\n",
    "            preds.append(res.forecast(1).values)\n",
    "            trues.append(df[col][i:i+1].values)\n",
    "        except:\n",
    "            pass\n",
    "RMSE = mse(np.array(preds),np.array(trues),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "121.989px",
    "width": "197.67px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
