

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>tsad.models &mdash; sklearn-template 12 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/project-template.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-rendered-html.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> sklearn-template
          

          
          </a>

          
            
            
              <div class="version">
                12
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start.html">Quick Start with the project-template</a></li>
</ul>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">tsad</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">sklearn-template</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>tsad.models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tsad.models</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<div class="viewcode-block" id="SimpleLSTM"><a class="viewcode-back" href="../../tsad.html#tsad.models.SimpleLSTM">[docs]</a><span class="k">class</span> <span class="nc">SimpleLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span>
                            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
                            <span class="n">num_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
                            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                            <span class="n">batch_first</span> <span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
                           <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_bidir</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">)</span>
    
    
<div class="viewcode-block" id="SimpleLSTM.initHidden"><a class="viewcode-back" href="../../tsad.html#tsad.models.SimpleLSTM.initHidden">[docs]</a>    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">k_bidir</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">k_bidir</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span></div>
<div class="viewcode-block" id="SimpleLSTM.forward"><a class="viewcode-back" href="../../tsad.html#tsad.models.SimpleLSTM.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequences</span><span class="p">):</span>
        <span class="n">batch_size</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="p">)</span>
        <span class="n">last_time_step</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># -1 is len_seq</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">last_time_step</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span></div>

<div class="viewcode-block" id="SimpleLSTM.run_epoch"><a class="viewcode-back" href="../../tsad.html#tsad.models.SimpleLSTM.run_epoch">[docs]</a>    <span class="k">def</span> <span class="nf">run_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">points_ahead</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span><span class="n">encod_decode_model</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">is_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">if</span> <span class="n">points_ahead</span> <span class="o">!=</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span><span class="n">points_ahead</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">points_ahead</span><span class="p">)</span><span class="o">==</span><span class="nb">type</span><span class="p">(</span><span class="nb">int</span><span class="p">()))</span>
            <span class="k">def</span> <span class="nf">forecast_multistep</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">points_ahead</span><span class="p">):</span>
                <span class="n">new_x</span> <span class="o">=</span> <span class="n">y_pred</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">points_ahead</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                    <span class="n">new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y_pred</span><span class="p">,</span><span class="n">new_x</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">y_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">forecast_multistep</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">points_ahead</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">y_pred</span>

        <span class="n">all_y_preds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
                <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1">#df.index rif of</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">initHidden</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
                <span class="n">y_true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forecast_multistep</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">points_ahead</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">encod_decode_model</span><span class="p">:</span>
                    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y_pred</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;forecast&#39;</span><span class="p">:</span>
                    <span class="n">all_y_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
                    <span class="k">continue</span> <span class="c1"># in case of pahse = &#39;forecast&#39; criterion is None</span>
                        
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_train</span><span class="p">:</span>
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                
                <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">!=</span> <span class="s1">&#39;forecast&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span><span class="c1">#, n_true_predicted / n_predicted</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_y_preds</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span></div></div>






<span class="c1">####################</span>
<span class="c1">#=======================================================</span>
<span class="c1">#              From https://github.com/Zhang-Zhi-Jie/Pytorch-MSCRED/blob/master/utils/matrix_generator.py</span>
<span class="c1">#=======================================================</span>







<span class="c1">#=======================================================</span>
<span class="c1">#              From other sourse</span>
<span class="c1">#=======================================================</span>
<span class="c1"># class ConvLSTMCell(nn.Module):</span>

<span class="c1">#     def __init__(self, input_dim, hidden_dim, kernel_size, bias):</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         MIT license</span>
<span class="c1">#         Authors and any other relevant information you can see:  </span>
<span class="c1">#         https://github.com/ndrplz/ConvLSTM_pytorch </span>
        
        
<span class="c1">#         Initialize ConvLSTM cell.</span>
<span class="c1">#         Parameters</span>
<span class="c1">#         ----------</span>
<span class="c1">#         input_dim: int</span>
<span class="c1">#             Number of channels of input tensor.</span>
<span class="c1">#         hidden_dim: int</span>
<span class="c1">#             Number of channels of hidden state.</span>
<span class="c1">#         kernel_size: (int, int)</span>
<span class="c1">#             Size of the convolutional kernel.</span>
<span class="c1">#         bias: bool</span>
<span class="c1">#             Whether or not to add the bias.</span>
<span class="c1">#         &quot;&quot;&quot;</span>

<span class="c1">#         super(ConvLSTMCell, self).__init__()</span>

<span class="c1">#         self.input_dim = input_dim</span>
<span class="c1">#         self.hidden_dim = hidden_dim</span>

<span class="c1">#         self.kernel_size = kernel_size</span>
<span class="c1">#         self.padding = kernel_size[0] // 2, kernel_size[1] // 2</span>
<span class="c1">#         self.bias = bias</span>

<span class="c1">#         self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,</span>
<span class="c1">#                               out_channels=4 * self.hidden_dim,</span>
<span class="c1">#                               kernel_size=self.kernel_size,</span>
<span class="c1">#                               padding=self.padding,</span>
<span class="c1">#                               bias=self.bias)</span>

<span class="c1">#     def forward(self, input_tensor, cur_state):</span>
<span class="c1">#         h_cur, c_cur = cur_state</span>

<span class="c1">#         combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis</span>

<span class="c1">#         combined_conv = self.conv(combined)</span>
<span class="c1">#         cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)</span>
<span class="c1">#         i = torch.sigmoid(cc_i)</span>
<span class="c1">#         f = torch.sigmoid(cc_f)</span>
<span class="c1">#         o = torch.sigmoid(cc_o)</span>
<span class="c1">#         g = torch.tanh(cc_g)</span>

<span class="c1">#         c_next = f * c_cur + i * g</span>
<span class="c1">#         h_next = o * torch.tanh(c_next)</span>

<span class="c1">#         return h_next, c_next</span>

<span class="c1">#     def init_hidden(self, batch_size, image_size):</span>
<span class="c1">#         height, width = image_size</span>
<span class="c1">#         return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),</span>
<span class="c1">#                 torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))</span>


<span class="c1"># class ConvLSTM(nn.Module):</span>

<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     MIT license</span>
<span class="c1">#     Authors and any other relevant information you can see:  </span>
<span class="c1">#     https://github.com/ndrplz/ConvLSTM_pytorch </span>
    
    
    
<span class="c1">#     Parameters:</span>
<span class="c1">#         input_dim: Number of channels in input</span>
<span class="c1">#         hidden_dim: Number of hidden channels</span>
<span class="c1">#         kernel_size: Size of kernel in convolutions</span>
<span class="c1">#         num_layers: Number of LSTM layers stacked on each other</span>
<span class="c1">#         batch_first: Whether or not dimension 0 is the batch or not</span>
<span class="c1">#         bias: Bias or no bias in Convolution</span>
<span class="c1">#         return_all_layers: Return the list of computations for all layers</span>
<span class="c1">#         Note: Will do same padding.</span>
<span class="c1">#     Input:</span>
<span class="c1">#         A tensor of size B, T, C, H, W or T, B, C, H, W</span>
<span class="c1">#     Output:</span>
<span class="c1">#         A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).</span>
<span class="c1">#             0 - layer_output_list is the list of lists of length T of each output</span>
<span class="c1">#             1 - last_state_list is the list of last states</span>
<span class="c1">#                     each element of the list is a tuple (h, c) for hidden state and memory</span>
<span class="c1">#     Example:</span>
<span class="c1">#         &gt;&gt; x = torch.rand((32, 10, 64, 128, 128))</span>
<span class="c1">#         &gt;&gt; convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)</span>
<span class="c1">#         &gt;&gt; _, last_states = convlstm(x)</span>
<span class="c1">#         &gt;&gt; h = last_states[0][0]  # 0 for layer index, 0 for h index</span>
<span class="c1">#     &quot;&quot;&quot;</span>

<span class="c1">#     def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,</span>
<span class="c1">#                  batch_first=False, bias=True, return_all_layers=False):</span>
<span class="c1">#         super(ConvLSTM, self).__init__()</span>

<span class="c1">#         self._check_kernel_size_consistency(kernel_size)</span>

<span class="c1">#         # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers</span>
<span class="c1">#         kernel_size = self._extend_for_multilayer(kernel_size, num_layers)</span>
<span class="c1">#         hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)</span>
<span class="c1">#         if not len(kernel_size) == len(hidden_dim) == num_layers:</span>
<span class="c1">#             raise ValueError(&#39;Inconsistent list length.&#39;)</span>

<span class="c1">#         self.input_dim = input_dim</span>
<span class="c1">#         self.hidden_dim = hidden_dim</span>
<span class="c1">#         self.kernel_size = kernel_size</span>
<span class="c1">#         self.num_layers = num_layers</span>
<span class="c1">#         self.batch_first = batch_first</span>
<span class="c1">#         self.bias = bias</span>
<span class="c1">#         self.return_all_layers = return_all_layers</span>

<span class="c1">#         cell_list = []</span>
<span class="c1">#         for i in range(0, self.num_layers):</span>
<span class="c1">#             cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]</span>

<span class="c1">#             cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,</span>
<span class="c1">#                                           hidden_dim=self.hidden_dim[i],</span>
<span class="c1">#                                           kernel_size=self.kernel_size[i],</span>
<span class="c1">#                                           bias=self.bias))</span>

<span class="c1">#         self.cell_list = nn.ModuleList(cell_list)</span>

<span class="c1">#     def forward(self, input_tensor, hidden_state=None):</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         Parameters</span>
<span class="c1">#         ----------</span>
<span class="c1">#         input_tensor: todo</span>
<span class="c1">#             5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)</span>
<span class="c1">#         hidden_state: todo</span>
<span class="c1">#             None. todo implement stateful</span>
<span class="c1">#         Returns</span>
<span class="c1">#         -------</span>
<span class="c1">#         last_state_list, layer_output</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         if not self.batch_first:</span>
<span class="c1">#             # (t, b, c, h, w) -&gt; (b, t, c, h, w)</span>
<span class="c1">#             input_tensor = input_tensor.permute(1, 0, 2, 3, 4)</span>

<span class="c1">#         b, _, _, h, w = input_tensor.size()</span>

<span class="c1">#         # Implement stateful ConvLSTM</span>
<span class="c1">#         if hidden_state is not None:</span>
<span class="c1">#             raise NotImplementedError()</span>
<span class="c1">#         else:</span>
<span class="c1">#             # Since the init is done in forward. Can send image size here</span>
<span class="c1">#             hidden_state = self._init_hidden(batch_size=b,</span>
<span class="c1">#                                              image_size=(h, w))</span>

<span class="c1">#         layer_output_list = []</span>
<span class="c1">#         last_state_list = []</span>

<span class="c1">#         seq_len = input_tensor.size(1)</span>
<span class="c1">#         cur_layer_input = input_tensor</span>

<span class="c1">#         for layer_idx in range(self.num_layers):</span>

<span class="c1">#             h, c = hidden_state[layer_idx]</span>
<span class="c1">#             output_inner = []</span>
<span class="c1">#             for t in range(seq_len):</span>
<span class="c1">#                 h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],</span>
<span class="c1">#                                                  cur_state=[h, c])</span>
<span class="c1">#                 output_inner.append(h)</span>

<span class="c1">#             layer_output = torch.stack(output_inner, dim=1)</span>
<span class="c1">#             cur_layer_input = layer_output</span>

<span class="c1">#             layer_output_list.append(layer_output)</span>
<span class="c1">#             last_state_list.append([h, c])</span>

<span class="c1">#         if not self.return_all_layers:</span>
<span class="c1">#             layer_output_list = layer_output_list[-1:]</span>
<span class="c1">#             last_state_list = last_state_list[-1:]</span>

<span class="c1">#         return layer_output_list, last_state_list</span>

<span class="c1">#     def _init_hidden(self, batch_size, image_size):</span>
<span class="c1">#         init_states = []</span>
<span class="c1">#         for i in range(self.num_layers):</span>
<span class="c1">#             init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))</span>
<span class="c1">#         return init_states</span>

<span class="c1">#     @staticmethod</span>
<span class="c1">#     def _check_kernel_size_consistency(kernel_size):</span>
<span class="c1">#         if not (isinstance(kernel_size, tuple) or</span>
<span class="c1">#                 (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):</span>
<span class="c1">#             raise ValueError(&#39;`kernel_size` must be tuple or list of tuples&#39;)</span>

<span class="c1">#     @staticmethod</span>
<span class="c1">#     def _extend_for_multilayer(param, num_layers):</span>
<span class="c1">#         if not isinstance(param, list):</span>
<span class="c1">#             param = [param] * num_layers</span>
<span class="c1">#         return param</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2016, Vighnesh Birodkar.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>